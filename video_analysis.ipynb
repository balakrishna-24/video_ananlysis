{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxyMPvR3WpX/GoSznuTntA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balakrishna-24/video_ananlysis/blob/main/video_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries!"
      ],
      "metadata": {
        "id": "prh4dbmADlYU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86kHjM7cDiD6",
        "outputId": "afc231b9-b8d6-48d2-aa3b-c69052810451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.8.6-py3-none-any.whl.metadata (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.1/170.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.8.30)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting websockets>=12.0 (from yt-dlp)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.10)\n",
            "Downloading yt_dlp-2024.8.6-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-13.1 yt-dlp-2024.8.6\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "\n",
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqsNHZkSDtEh",
        "outputId": "49f44454-2b20-4c01-d15c-b84cff0d5027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16960, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 16960 (delta 79), reused 95 (delta 45), pack-reused 16805 (from 1)\u001b[K\n",
            "Receiving objects: 100% (16960/16960), 15.71 MiB | 17.78 MiB/s, done.\n",
            "Resolving deltas: 100% (11612/11612), done.\n",
            "/content/yolov5\n",
            "Collecting gitpython>=3.1.30 (from -r requirements.txt (line 5))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (10.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.5)\n",
            "Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.2.98-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.1)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (71.0.4)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r requirements.txt (line 5))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.6.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n",
            "  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading ultralytics-8.2.98-py3-none-any.whl (873 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m873.6/873.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, ultralytics-thop, thop, gitpython, ultralytics\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1 thop-0.1.1.post2209072238 ultralytics-8.2.98 ultralytics-thop-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages if not already available\n",
        "!pip install torch torchvision\n",
        "\n",
        "!pip install pytube whisper openai gtts\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Dco9vKDw9j",
        "outputId": "3ca780c1-ba16-43a2-83e8-74935c437853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai\n",
            "  Downloading openai-1.47.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.47.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=ff4d52069ef2f068895a540dae2100b2f369f1fbcba02a660d4730bda09d5d72\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/7c/1d/015619716e2facae6631312503baf3c3220e6a9a3508cb14b6\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper, pytube, jiter, h11, httpcore, gtts, httpx, openai\n",
            "Successfully installed gtts-2.5.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.47.0 pytube-15.0.0 whisper-1.1.10\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-luuqt9sa\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-luuqt9sa\n",
            "  Resolved https://github.com/openai/whisper.git to commit 279133e3107392276dc509148da1f41bfb532c7e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20231117)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20231117) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802819 sha256=98e44bbb13b461244f242fd24acda0d96dd083e68c91b5220a792ec077a733ba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xf7278_p/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.7.0 triton-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3D4au_-D71a",
        "outputId": "95780602-0891-428c-dcf5-48cf7ac20919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.47.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import imutils\n",
        "import time\n",
        "import hashlib\n",
        "from yt_dlp import YoutubeDL\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import os\n",
        "import whisper\n",
        "import openai  # Install with: pip install openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'Your_api'  # Replace with your actual OpenAI API key\n",
        "\n",
        "# Load models\n",
        "# Load YOLOv5 model for object detection\n",
        "model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Load Places365 model for scene classification\n",
        "model_places = models.resnet50(pretrained=False)\n",
        "model_places.fc = torch.nn.Linear(model_places.fc.in_features, 365)\n",
        "checkpoint = torch.hub.load_state_dict_from_url(\n",
        "    'http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar',\n",
        "    map_location=torch.device('cpu')\n",
        ")\n",
        "state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "model_places.load_state_dict(state_dict)\n",
        "model_places.eval()\n",
        "\n",
        "# Define transformation for scene classification\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Load Places365 scene labels\n",
        "LABELS_URL = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
        "response = requests.get(LABELS_URL)\n",
        "categories = [line.strip().split(' ')[0][3:] for line in response.text.splitlines()]\n",
        "\n",
        "# Load Whisper model for audio transcription\n",
        "model_whisper = whisper.load_model(\"base\")\n",
        "\n",
        "# Function to download YouTube video\n",
        "def download_youtube_video(url):\n",
        "    url_hash = hashlib.md5(url.encode()).hexdigest()\n",
        "    output_path = f'video_{url_hash}.mp4'\n",
        "    ydl_opts = {\n",
        "        'format': 'best',\n",
        "        'outtmpl': output_path,\n",
        "    }\n",
        "    try:\n",
        "        with YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "        print(f\"Downloaded video to {output_path}\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract frames from the video\n",
        "def get_frames(video_path, capture_interval=20):\n",
        "    vs = cv2.VideoCapture(video_path)\n",
        "    if not vs.isOpened():\n",
        "        raise Exception(f'Unable to open file {video_path}')\n",
        "\n",
        "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    frame_count = 0\n",
        "    frame_time = 0\n",
        "\n",
        "    skip_frames = int(fps * capture_interval)\n",
        "\n",
        "    while True:\n",
        "        vs.set(cv2.CAP_PROP_POS_FRAMES, frame_count * skip_frames)\n",
        "        success, frame = vs.read()\n",
        "        if not success:\n",
        "            break\n",
        "        yield frame_count, frame_time, frame\n",
        "        frame_count += 1\n",
        "        frame_time += capture_interval\n",
        "\n",
        "    vs.release()\n",
        "\n",
        "# Object detection function using YOLOv5\n",
        "def detect_objects(frame):\n",
        "    results = model_yolo(frame)\n",
        "    detected_objects = results.pandas().xyxy[0]['name'].tolist()\n",
        "    return detected_objects\n",
        "\n",
        "# Scene classification function using Places365\n",
        "def classify_scene(frame):\n",
        "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model_places(img_tensor)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    top5_probs, top5_cats = torch.topk(probs, 5)\n",
        "    top_scene = categories[top5_cats[0][0]]\n",
        "    return top_scene\n",
        "\n",
        "# Function to extract and transcribe audio using Whisper\n",
        "def extract_and_transcribe_audio(video_url):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': 'audio.%(ext)s',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([video_url])\n",
        "\n",
        "    audio_file = 'audio.mp3'\n",
        "    if os.path.exists(audio_file):\n",
        "        print(f\"Audio file downloaded: {audio_file}\")\n",
        "        result = model_whisper.transcribe(audio_file)\n",
        "        transcription = result['text']\n",
        "        print(\"Transcription:\\n\", transcription)\n",
        "        return transcription\n",
        "    else:\n",
        "        print(\"Audio file not found!\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to process frames and collect visual data\n",
        "def process_frames(video_path, capture_interval=20):\n",
        "    visual_data = []\n",
        "    for frame_count, frame_time, frame in get_frames(video_path, capture_interval):\n",
        "        # Resize the frame for faster processing\n",
        "        frame_resized = imutils.resize(frame, width=600)\n",
        "\n",
        "        # Object detection\n",
        "        print(f\"Frame {frame_count}: Performing object detection\")\n",
        "        detected_objects = detect_objects(frame_resized)\n",
        "\n",
        "        # Scene classification\n",
        "        print(f\"Frame {frame_count}: Performing scene classification\")\n",
        "        top_scene = classify_scene(frame_resized)\n",
        "\n",
        "        # Collect visual information\n",
        "        visual_info = {\n",
        "            'frame_count': frame_count,\n",
        "            'frame_time': frame_time,\n",
        "            'scene': top_scene,\n",
        "            'detected_objects': detected_objects,\n",
        "        }\n",
        "        visual_data.append(visual_info)\n",
        "        print(f\"Processed frame {frame_count} with scene '{top_scene}' and objects {detected_objects}\")\n",
        "\n",
        "    print(f'{len(visual_data)} frames processed with object detection and scene classification!')\n",
        "    return visual_data\n",
        "\n",
        "# Function to pass the combined data to the LLM\n",
        "def analyze_with_llm(visual_data, audio_transcription):\n",
        "    # Prepare the prompt\n",
        "    prompt = \"You are provided with the following visual and audio information extracted from a video.\\n\\n\"\n",
        "\n",
        "    # Add visual data to the prompt\n",
        "    prompt += \"Visual Information:\\n\"\n",
        "    for info in visual_data:\n",
        "        prompt += f\"Frame {info['frame_count']} at {info['frame_time']}s:\\n\"\n",
        "        prompt += f\"- Scene: {info['scene']}\\n\"\n",
        "        detected_objects = ', '.join(info['detected_objects']) if info['detected_objects'] else 'None'\n",
        "        prompt += f\"- Detected Objects: {detected_objects}\\n\\n\"\n",
        "\n",
        "    # Add audio transcription to the prompt\n",
        "    prompt += \"Audio Transcription:\\n\"\n",
        "    prompt += audio_transcription.strip() + \"\\n\\n\"\n",
        "\n",
        "    prompt += \"Based on the above information, please provide a detailed analysis and explanation of the video's content, including any themes, events, or messages conveyed.\"\n",
        "\n",
        "    # Prepare messages for the ChatCompletion API\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Call the OpenAI ChatCompletion API\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4\",  # Or use \"gpt-3.5-turbo\" if you prefer\n",
        "        messages=messages,\n",
        "        max_tokens=500,   # Adjust as needed\n",
        "        temperature=0.7   # Adjust for creativity\n",
        "    )\n",
        "\n",
        "    analysis = response.choices[0].message.content\n",
        "    return analysis\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Please enter the YouTube video URL: \").strip()\n",
        "    if video_url:\n",
        "        print('Processing video_url:', video_url)\n",
        "\n",
        "        # Step 1: Download the video\n",
        "        video_path = download_youtube_video(video_url)\n",
        "        if video_path:\n",
        "            # Step 2: Process frames and collect visual data\n",
        "            visual_data = process_frames(video_path, capture_interval=20)\n",
        "\n",
        "            # Step 3: Extract and transcribe audio\n",
        "            audio_transcription = extract_and_transcribe_audio(video_url)\n",
        "\n",
        "            # Step 4: Combine visual and audio data and analyze with LLM\n",
        "            analysis = analyze_with_llm(visual_data, audio_transcription)\n",
        "            print(\"\\nLLM Analysis and Explanation:\\n\")\n",
        "            print(analysis)\n",
        "        else:\n",
        "            print(\"Failed to download video.\")\n",
        "    else:\n",
        "        print(\"Invalid URL entered.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg0BVs2bEP6r",
        "outputId": "a30f6fc7-dc5e-4660-8152-6c8aa682c251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:295: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2024-9-20 Python-3.10.12 torch-2.4.1+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 44.6MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\" to /root/.cache/torch/hub/checkpoints/resnet50_places365.pth.tar\n",
            "100%|██████████| 92.8M/92.8M [00:06<00:00, 15.4MB/s]\n",
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 52.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the YouTube video URL: https://www.youtube.com/watch?v=Lx3gSNScFJA\n",
            "Processing video_url: https://www.youtube.com/watch?v=Lx3gSNScFJA\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Lx3gSNScFJA\n",
            "[youtube] Lx3gSNScFJA: Downloading webpage\n",
            "[youtube] Lx3gSNScFJA: Downloading ios player API JSON\n",
            "[youtube] Lx3gSNScFJA: Downloading web creator player API JSON\n",
            "[youtube] Lx3gSNScFJA: Downloading player a9d81eca\n",
            "[youtube] Lx3gSNScFJA: Downloading m3u8 information\n",
            "[info] Lx3gSNScFJA: Downloading 1 format(s): 18\n",
            "[download] Destination: video_fe7983658d3fd78d6c2a815640d24a1c.mp4\n",
            "[download] 100% of   21.57MiB in 00:00:02 at 7.90MiB/s   \n",
            "Downloaded video to video_fe7983658d3fd78d6c2a815640d24a1c.mp4\n",
            "Frame 0: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 0: Performing scene classification\n",
            "Processed frame 0 with scene 'television_studio' and objects ['tie', 'person']\n",
            "Frame 1: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1: Performing scene classification\n",
            "Processed frame 1 with scene 'television_studio' and objects ['person', 'tie', 'person']\n",
            "Frame 2: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2: Performing scene classification\n",
            "Processed frame 2 with scene 'television_studio' and objects ['person', 'person', 'person', 'tie', 'tie', 'potted plant']\n",
            "Frame 3: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 3: Performing scene classification\n",
            "Processed frame 3 with scene 'art_gallery' and objects ['person', 'person', 'person', 'tv']\n",
            "Frame 4: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 4: Performing scene classification\n",
            "Processed frame 4 with scene 'art_gallery' and objects ['person', 'person', 'person', 'tv']\n",
            "Frame 5: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 5: Performing scene classification\n",
            "Processed frame 5 with scene 'television_studio' and objects ['tie', 'person', 'cell phone']\n",
            "Frame 6: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 6: Performing scene classification\n",
            "Processed frame 6 with scene 'television_studio' and objects ['person', 'person', 'tie', 'person', 'tie', 'tv', 'tie', 'potted plant']\n",
            "Frame 7: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 7: Performing scene classification\n",
            "Processed frame 7 with scene 'television_studio' and objects ['person', 'person', 'tie', 'person', 'tie', 'person']\n",
            "Frame 8: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 8: Performing scene classification\n",
            "Processed frame 8 with scene 'conference_center' and objects ['person', 'tie']\n",
            "Frame 9: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 9: Performing scene classification\n",
            "Processed frame 9 with scene 'television_studio' and objects ['person', 'tie', 'person', 'tie', 'person']\n",
            "Frame 10: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 10: Performing scene classification\n",
            "Processed frame 10 with scene 'television_studio' and objects ['person', 'tie']\n",
            "Frame 11: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 11: Performing scene classification\n",
            "Processed frame 11 with scene 'television_studio' and objects ['person', 'person', 'tie', 'tv', 'tie', 'potted plant']\n",
            "Frame 12: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 12: Performing scene classification\n",
            "Processed frame 12 with scene 'television_studio' and objects ['person', 'person', 'person', 'tie', 'tie', 'tie', 'tv', 'potted plant']\n",
            "Frame 13: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 13: Performing scene classification\n",
            "Processed frame 13 with scene 'television_studio' and objects ['person', 'tie']\n",
            "Frame 14: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 14: Performing scene classification\n",
            "Processed frame 14 with scene 'television_studio' and objects ['person', 'person', 'tie', 'person', 'tie', 'tie', 'tv']\n",
            "Frame 15: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 15: Performing scene classification\n",
            "Processed frame 15 with scene 'television_studio' and objects ['person', 'person', 'person', 'tie', 'tie', 'tie', 'potted plant']\n",
            "Frame 16: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 16: Performing scene classification\n",
            "Processed frame 16 with scene 'television_studio' and objects ['person', 'person', 'person', 'tie', 'tie', 'tie', 'tv', 'potted plant']\n",
            "Frame 17: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 17: Performing scene classification\n",
            "Processed frame 17 with scene 'beauty_salon' and objects ['person', 'person', 'person', 'potted plant']\n",
            "Frame 18: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 18: Performing scene classification\n",
            "Processed frame 18 with scene 'television_studio' and objects ['person', 'tie', 'person']\n",
            "Frame 19: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 19: Performing scene classification\n",
            "Processed frame 19 with scene 'television_studio' and objects ['person', 'person', 'tie', 'person', 'tie', 'tv', 'tie']\n",
            "Frame 20: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 20: Performing scene classification\n",
            "Processed frame 20 with scene 'television_studio' and objects ['person', 'person', 'tie', 'person', 'tie', 'tv', 'potted plant']\n",
            "Frame 21: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 21: Performing scene classification\n",
            "Processed frame 21 with scene 'television_studio' and objects ['person', 'person', 'tie', 'person', 'tie', 'tv', 'tie', 'potted plant']\n",
            "Frame 22: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 22: Performing scene classification\n",
            "Processed frame 22 with scene 'television_studio' and objects ['person', 'tie', 'person', 'tie', 'person']\n",
            "Frame 23: Performing object detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 23: Performing scene classification\n",
            "Processed frame 23 with scene 'television_studio' and objects ['person', 'tie', 'tv']\n",
            "24 frames processed with object detection and scene classification!\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Lx3gSNScFJA\n",
            "[youtube] Lx3gSNScFJA: Downloading webpage\n",
            "[youtube] Lx3gSNScFJA: Downloading ios player API JSON\n",
            "[youtube] Lx3gSNScFJA: Downloading web creator player API JSON\n",
            "[youtube] Lx3gSNScFJA: Downloading m3u8 information\n",
            "[info] Lx3gSNScFJA: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "[download] 100% of    6.74MiB in 00:00:00 at 9.36MiB/s   \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n",
            "Audio file downloaded: audio.mp3\n",
            "Transcription:\n",
            "  By the way, the Wall Street Journal is reporting when asked about whether Donald Trump was, in fact, ruling out a debate. And then we're playing games here. Spokeswoman simply said in an email, you heard the man. He's not debating. All right. A duck show in the former Clinton advisor, a Democratic strategist and pollster. No, I'm Nick Porridge, your UP fundraiser. No, if you don't mind, I'd like to begin with you first and the fallout. I mean, you talk to big donors and the Republican Party. You're well connected to those and the Republican Party. So from a political standpoint, is it better in their mind than Donald Trump not debate again that that that performance he had with Kamala Harris was such that it might not be such a good idea? What do you think? A hundred percent on that. I think a lot of the donors are at least the ones that I was texting with and I did the debate were not exactly thrilled about how that debate went down. But I will tell you, I do think it's smart for Donald Trump not to do the debate. I think that that was a good decision with the campaign and I'll tell you why because depending on when that debate will be scheduled and if he has another repeat performance like he did, I don't know if the campaign can recover that quickly. I don't think they have enough time to recover from that. So you've got to keep in mind, we've got 54 days before the election. And if there is another, by the time the debate's set and by the time it happens, you've got to have ads ready, you've got to be able to respond to anything that would be negative. So I think due to the timing, the debate not doing it, I think was a wise decision. You know Doug, you remind me, you're such a good historian as well on these debates and having coached and worked with the candidates in the past on them that we sometimes give them sort of undue importance and vitality. And maybe with the exception of John F. Kennedy against Richard Nixon in that opening debate, they had four. It doesn't really change the needle much. I'm thinking maybe with Gerald Ford and Jimmy Carter and of course President Ford at the time saying that Eastern Europe wasn't under communist rule, it can come back to snack too. But by and large, they don't make a difference. By and large. How do you see it? And that's right now. I think tops this debate will temporarily get Kamala Harris and extra pointed or two, maybe three, but it'll be temporary. And Trump doesn't need, excuse me, another debate. He needs a campaign. And since he didn't do well, Noel was right. There's no reason to debate. And I think he wants to go run his campaign and demonstrate as he did in 2016 and 2020 that he's a great closer and he closed not with debate performance, but he closed with his rallies and is on the ground performance where he exceeded the final polls. So I think his campaign feels their way ahead of the game that they have had so far, only a narrow slippage and bottom line. You're not going to see him on the debate stage. It's the right decision for him. And Kamala Harris will go to him. I don't think it's going to work. You know, I'm reading our course when you hear from Donald Trump guys and when he says about this, all the polls show that I won that debate. It's simply fair to say that consensus shows that she got the better of him. I get people kind of mixed views on this. I understand that. But no, well, one of the things I've discovered that even some of the former presidents' closest allies have been saying, you know, Lindsey Graham on the go to done better. I'm paraphrasing here. But the most, you know, I think startling to be a came from Robert Kennedy, Jr. was a guest on this show. And of course, he is supporting Donald Trump. Might have a role in a Trump administration. I want you to react to this. President Harris clearly won the debate in terms of her delivery, her polish, her organization and her preparation. I think on the substance, President Trump wins in terms of his governance. I would suspect that the polling over the next week is going to show, you know, probably a slight drop in his support, particularly among independence. All right. And he's a key supporter, as you know, Noel. And that we're told that a number, you know, from confidence, we're sort of being blunt and honest to say, all right, you know, you brought your A game when you debated, Joe Biden, this wasn't your A game. And that might be among the more polite critiques that he's had. Do you think he bounced this decision, or some of those same, you know, confidence, or that he just went into this on his own? Well, if you're talking about RFK and what he said, I don't think he will have a role anymore going forward. If this is what he came out, it said that that he thought that he lost the debate. It wasn't his best debate and that he may have lost independent votes. That is not the Trump campaign spin. The Trump campaign spin is it basically it was three against one that the moderators and Miss Harris were totally against him. They headed out for him and that if he asked everybody looking at everything that he has, that he's totally came out ahead and he won the debate and why, you know, that she wants another debate because she felt like she lost and he won. That is the Trump spin. Trump never admits anything negative. Trump is always going to say that he came out the winner no matter what this situation came out. And he is the winner. And this is how, you know, Trump and the Trump campaign spins it. So the sheer fact that somebody is a surrogate and somebody is, you know, kind of part of the Trump campaign campaign cap, meaning RFK Jr. saying that he did not win. I would not look for RFK Jr. to have any role if Donald Trump wins. Yeah. And whether he's listening to those confidants as well and others, even on this decision that shown not to debate. He did bring up something that others have that the moderators might not have treated him fairly. And he constantly fact checked him that certainly Kamala Harris was not on a couple of key things, including Donald Trump and whether, you know, he had support for this project 2025. He's continuously disavowed that and also his views on the vitro fertilization far from separating himself from it. He's proposed funding it for couples. And even the fact that there are no American troops in combat zones overseas, not a one was second guess by the moderators. So does he have a point, or does that even matter at this point? Yeah, he does have a point. And that would enter Neil into his decision not to debate anymore because he knows Kamala Harris is not going to come on Fox News. She wants to go mainstream media and he can take the risk that three on one. And he does have a point. It's a fair point. All right, guys, I want to thank you very much. Again, we will be hearing from Donald Trump. He'll be speaking in Arizona shortly the next half hour or so. And he might elaborate a little what was behind that decision to say no to another debate with Kamala Harris. Hey, Sean Hannity here. Hey, click here to subscribe to Fox News YouTube page and catch our hottest interviews. And most compelling analysis, you will not get it anywhere else.\n",
            "\n",
            "LLM Analysis and Explanation:\n",
            "\n",
            "The video appears to be a news commentary or political analysis show, most likely broadcasted on a network like Fox News, considering the reference to Sean Hannity, a known Fox News commentator. The visual information indicates that the video takes place in a television studio for most of the duration, with brief switches to an art gallery, a conference center, and a beauty salon. The images also indicate multiple people present, with the number of people varying throughout the video.\n",
            "\n",
            "The main theme of the audio transcription is a discussion about the U.S. Presidential debates, specifically focusing on Donald Trump's decision not to participate in further debates with Kamala Harris. The speakers analyze whether this decision is beneficial for Trump's campaign, based on his previous debate performance. There seem to be mixed opinions, with some suggesting that the decision is wise considering the negative response to his performance in the previous debate, while others suggest that this might hurt his campaign.\n",
            "\n",
            "The participants also discuss the role of debate moderators and whether they have been fair in their treatment of Trump. Some suggest that Trump's decision not to participate in further debates might be influenced by perceived bias from the moderators. \n",
            "\n",
            "There is also discussion about potential fallout within Trump's campaign and among his supporters. Opinions vary on whether Trump's decision will impact his support base, particularly among independent voters. One speaker suggests that a key supporter, Robert Kennedy Jr., may lose his role in the Trump administration due to his comments that Trump lost the debate.\n",
            "\n",
            "Overall, the video seems to be an in-depth analysis of Trump's political strategy and the potential impacts of his decision not to participate in further debates. The speakers provide different perspectives, allowing viewers to form their own opinion on the matter.\n"
          ]
        }
      ]
    }
  ]
}